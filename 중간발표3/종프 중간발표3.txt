저희는 “대규모 언어 모델”을 이용한 서비스에서 “강의”라는 주제로 프로젝트를 수행하고 있는 13팀이고, 저는
발표를 맡은 조정래입니다.
목차는 수행배경 및 목표, 시스템 요구 분석 및 정의, 개발과정, 프로토타입 마지막으로 향후 계획 순으로 발
표하겠습니다.
먼저 수행배경 및 목표입니다.
다들 방학 때, 대학생들이라면 항상 수강신청이라는 고민에 빠질텐데요, 한 학기를 알차게 보내기 위해 에브리
타임에 접속해서 여러 강의, 교수님들을 검색해본 경험이 있으실 겁니다.
저희는 이 때, 내가 들을 수업을 찾는 수고를 덜고자, 효율적으로 강의를 알아보고 나한테 맞는 강의를 추천받
는 서비스를 고안해내게 되었습니다.
다음으로 시스템 요구 분석 및 정의입니다.
저희 시스템은 frontend는 react.js, backend는 flask framework로 구성했습니다.
서버는 Google Cloud에 있는 Google Cloud Platform을 이용해서 구축했고 나머지 구성들은 이후 발표에서 설
명하도록 하겠습니다.
개발과정입니다.
먼저 저희가 첫번째로 구상한 시스템입니다.
사용자로부터 입력이 들어오면 입력에서 강의명, 교수명을 슬라이싱해서 가져온 다음, DB에 query를 하고, DB
에서 가져온 강의평들을 LLM에 사용자의 입력과 같이 입력해서 받은 response를 사용자에게 출력해주는 방식
입니다.
LLM 같은 경우는 Google Cloud Vertex AI에 있는 Chat-bison model을 사용했고, DB의 경우엔 Google의
firebase realtime Datebase를 사용했습니다. DB에는 저희가 먼저 에브리타임에서 크롤링했던 컴퓨터학부 과목,
교수님들의 강의평들을 저장해놨습니다.
이 시스템에는 문제점이 있는데요, 여러분들이 chat-gpt를 이용해서 채팅을 해보면, 예를 들어 정설영 교수님
어때? 라는 질문을 하면 수업 잘가르치시고, 시험 잘내시고 교수님께서 되게 열정적이시다. 이렇게 답변할 것
입니다. 그러고 나서 그럼 그 수업 과제는 많아? 라는 질문을 할 수 있을 것입니다. 이 때, LLM은 이전에 받은
질문과 답변을 기억하고, 이를 토대로 이 질문에 답변할 수 있어야 하는데요. 이를 history 기능이라고 합니다.
저희 시스템은 근데 사용자의 질문으로부터 과목명과 교수명을 슬라이싱해서 사용하다보니, 두번째 질문같은
경우 과목명과 교수명을 가져올 수 없었고, 따라서 DB에 query 할 수도 없다보니 LLM이 아무런 강의평 정보
없이 답변을 하게되는 문제점이 발생했습니다.
그래서 구상한 시스템이 다음 시스템입니다.
이 시스템에서 저희는 사용자의 입력이 들어오면, LLM에 입력해서 과목명, 교수명을 반환받도록 했습니다. 받
환받은 과목명, 교수명을 토대로 DB에 query를 진행하는데요, 이 때, 만약 과목명, 교수명이 반환되지 않는다
면 jailbreaking이라고 판단해 다음 단계로 진행하지 않도록 설계했습니다.
jailbreaking이란 LLM에서 중요한 요소인데요, LLM은 사용자의 입력을 받아 response 하는 인공지능 모델입니
다. 하지만 만약 사용자의 입력이 반인류적이고, 불법적인 질문이라면 LLM이 답변을 해주면 안될 것입니다.
예를 들어 마약 어떻게 구매해?라는 질문이 들어오면 LLM은 이 질문에 답변해주면 안되겠죠? 이를 위해 서비
스를 개발할 때, jailbreaking을 신경써서 개발해야합니다.
이 시스템에도 문제점이 있었는데요. 바로 효율적이지 않다는 문제점입니다.
LLM을 호출하는건 결국 overhead 문제로 이어진다는 문제점이 있습니다.
또, 일반 DB에 질의하는 경우 만약 교수님께서 강의하신 수업이 많은 경우, 수많은 강의 데이터들을 가져오게
될 것인데, 그 때 LLM의 Maximum input token 양을 초과해서 LLM이 답변을 하지 못하는 문제점이 발생합니
다.
그리고 강의평에 들어있는 prompt engineering에 적합하지 않은 noise 데이터들이 LLM에 제공되어 효과적인
답변이 나오지 않게 됩니다.
그래서 개발한 시스템이 현재 저희 팀의 시스템이자 저희 팀의 프로토타입입니다.
저희 프로토타입에선 아까의 시스템에서 DB만 VectorDB로 바꾸어주었습니다.
사용자의 입력을 첫번째 LLM에 넣을 때, overhead를 줄일 수 없어서 저희는 어차피 호출하는거 최대한 많은
정보를 끌어내자라고 생각해, 과목명, 교수명 그리고 질문 내용을 LLM에게 분석해주게 했습니다. 그리고 LLM
의 output을 이용해 query를 짜서 VectorDB에 질의를 하게 됩니다.
VectorDB에 대해 궁금하신 분들이 많으실텐데요, VectorDB는 일반적인 DB에 vector라는 독특한 data type을
제공하는 database를 말합니다. 그리고 이 vector끼리의 연산 또한 제공하는데요, 이 연산을 통해 vector 끼리
의 비교가 가능합니다. 여기서 RAG 기법이 사용되는데요, 이 RAG 기법은 최근에 발표된 GPT-4에서도 사용되
는 LLM에 없어서는 안될 핵심 기술입니다.
RAG 기법이 뭐냐면 여러분들이 “인사”라는 말을 생각해 보면 안녕, 잘가, 잘지내, 굿모닝, 좋은 아침 등을 떠
올리실텐데요. RAG 기법이 없다면 인사 라는 말을 컴퓨터에 입력한다면 “인사”라는 단어가 들어간 문장들만
가져올 것입니다. 하지만 RAG기법을 이용하면 안녕, 잘가, 잘지내, 굿모닝 등의 사람들이 느끼기에 인사와 비
슷한 단어들을 모두 가져올 수 있습니다.
이 기법을 사용하기 위해 저희는 에브리타임의 강의평들을 모두 임베딩해서 VectorDB에 vector 형태로 저장해
두었고, 사용자로부터 질문이 들어오면 그 질문을 임베딩해서 VectorDB에 저장되어 있는 vector들과 연산을
합니다. 연산 결과 의미가 가장 비슷한 30개의 강의평들을 가져와서 2번째 LLM의 context에 다른 추가적인
지시사항과 함께 입력하고 받은 response를 사용자에게 출력해주는 방식입니다.
연산은 L2 Distance, Cosine Distance, inner product 3가지 연산이 있는데, L2 distance는 간단히 말해 두 벡터
사이의 거리이고, cosine distance는 두 벡터 사이에 끼인 각 theta의 cosine 값입니다. inner product는 내적입
니다. 저희는 Cosine distance를 이용해 data를 30개 추출해냈고, 이를 통해 LLM의 Maximum input token
Limitation을 해결할 수 있었습니다.
이 시스템에서 저희는 VectorDB로는 Google Cloud에 있는 PostgreSQL의 pgVector Database를 사용했습니다.
현재 저희 프로토타입에 문제점이 있는데요, 바로 과목 / 교수 / 과목 + 교수명이 들어있는 한정적인 질문 형
식에만 답변할 수 있다는 점이고, 인공지능 수업 추천해줘와 같은 강의 추천은 안되고 강의평 요약만 가능하
다는 문제점이 있습니다. 그리고, 프로토타입을 서버에 배포하고 보니 로컬 환경에서는 사용자가 한명이라 한
chat-model만 이용해도 history기능을 구현하는데 문제가 없었지만, 서버에서 다중 사용자가 서비스를 이용하
면 user끼리의 질문이 겹쳐 LLM에서 history 기능이 제대로 동작하지 않는 문제점 또한 있습니다.
이와 같은 문제점들을 해결하기 위해 저희는 향후 계획을 세웠습니다.
먼저, session을 관리해서 LLM의 history 기능을 구현하고자 합니다.
사용자가 저희 서비스에 접속하면 사용자별로 session을 부여해서 자신만의 서비스를 제공받을 수 있도록 합
니다.
session 기능이 구현되면 컴퓨터학부 학우분들을 대상으로 희망자에 한해 설문조사를 진행하려고 합니다. 설문
조사를 통해 현재 서비스의 답변의 질은 어떤지? 또 추가적으로 어떤 기능이 있으면 좋을지 등에 대해 수집하
고, 수집된 정보들을 이용해 가능한 기능들은 최대한 추가하고, prompt engineering을 통해 답변의 질을 향상
시키고자 합니다.
발표는 여기까지이고, 혹시나 궁금하신 점이 있다면 편하게 질문해주시면 감사하겠습니다.